{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"try_h5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qUOghuRyPROE","executionInfo":{"status":"ok","timestamp":1653762690569,"user_tz":-540,"elapsed":18187,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"f3a8a786-95b5-405f-92c0-cd08a6df6410"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["버전 다운그레이드를 해야 하여, tensorflow를 uninstall 합니다."],"metadata":{"id":"wGUhBUTrP2RB"}},{"cell_type":"code","source":["pip uninstall tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZwVRqvmzllE","executionInfo":{"status":"ok","timestamp":1651017947577,"user_tz":-540,"elapsed":24901,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"5fdd48e9-ba09-4b7b-97b1-a8acf1993b85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.7/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n","Proceed (y/n)? ㅛ\n","Your response ('ㅛ') was not one of the expected responses: y, n\n","Proceed (y/n)? ㅛ\n","Your response ('ㅛ') was not one of the expected responses: y, n\n","Proceed (y/n)? y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}]},{"cell_type":"markdown","source":["1.15버전부터 호환 되는 함수가 있어서, 1.15.0 버전을 불러옵니다."],"metadata":{"id":"JaSw3djMP9Np"}},{"cell_type":"code","source":["!pip install tensorflow==1.2.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4hqp-kFEzq5G","executionInfo":{"status":"ok","timestamp":1651018027146,"user_tz":-540,"elapsed":56153,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"c5e53a29-2c36-47f6-982f-7fbfafeb6bb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==1.15.0\n","  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n","\u001b[K     |████████████████████████████████| 412.3 MB 23 kB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 4.6 MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.44.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 33.1 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.6)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 30.1 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.2.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=572c36c671cffcfd739f9b2caec9e43d377cd20597a45defc577e208debe69de\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow"]}}},"metadata":{}}]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"leQF2T6yhu8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"R6FgfhCY0O6Y","executionInfo":{"status":"ok","timestamp":1653762067847,"user_tz":-540,"elapsed":296,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"a5c410b1-059f-4aae-e9f6-e0dd2c74c3ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.8.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["재준님이 하셨던 것처럼, 파이썬 파일에 접근하기 위해 경로를 찾아 갑니다."],"metadata":{"id":"t04keCtQQEmS"}},{"cell_type":"code","source":["cd /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rgsC3Gm67wL","executionInfo":{"status":"ok","timestamp":1653762456383,"user_tz":-540,"elapsed":310,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"401c8d48-6dd1-4bec-9b08-269d4964b99b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["cd drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijrtnAzpPXxz","executionInfo":{"status":"ok","timestamp":1653762724876,"user_tz":-540,"elapsed":1221,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"9c331a4a-9cd2-417d-bafc-45518a5168ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hST31Ra0QOvy","executionInfo":{"status":"ok","timestamp":1653762075215,"user_tz":-540,"elapsed":5,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"d059ec31-1541-411a-96b0-7e61765f5a45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34malbert-module\u001b[0m/                \u001b[01;34mlayers\u001b[0m/                    requirements.txt\n","\u001b[01;34mdata\u001b[0m/                         log                        \u001b[01;34msave_model\u001b[0m/\n","\u001b[01;34mdata_text\u001b[0m/                    \u001b[01;34mmodels\u001b[0m/                    test_pv.sh\n","eval_joint_bert_allsents.py   prepare_data_mecab_bpe.py  train_joint_bert.py\n","eval_joint_bert_allsents.py~  \u001b[01;34m__pycache__\u001b[0m/               tutorial\n","eval_joint_bert.py            \u001b[01;34mreaders\u001b[0m/                   utils.py\n","eval_joint_bert.py~           README.md                  \u001b[01;34mvectorizers\u001b[0m/\n"]}]},{"cell_type":"markdown","source":["## 이제 재준님이 시도하셨던 .py 불러오기"],"metadata":{"id":"010LzofJQQab"}},{"cell_type":"code","source":["pip install keras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDzoUW3d6A-z","executionInfo":{"status":"ok","timestamp":1653762225325,"user_tz":-540,"elapsed":3000,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"d732216c-23c9-4080-a674-3191c34f968a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"]}]},{"cell_type":"code","source":["from models import joint_bert_fixed # models 폴더 안에, joint_bert.py 불러오기"],"metadata":{"id":"gRWDDp56QPFv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 이제 우리가 하려고 했던 부분을 하는 과정에서, 원래 과정은\n","- keras의 load함수를 이용한다. -> 그 과정에서 layer가 필요해서, alber_layer를 불러온다. 였는데, `Tensorflow 버전 1` 에서는, load함수를 불러 올 수가 없었습니다.\n","\n","- 그래서 확인을 해 보니, models 폴더에 있는, joint_bert.py 내 클래스에, load 함수가 따로 있어서, 이를 이용하여 .h5 를 불러오는 방식을 채택하였습니다."],"metadata":{"id":"CVAocGPcQZ1g"}},{"cell_type":"code","source":["# from layers import albert_layer  # layers 폴더 안에, albert_layer.py 불러오기"],"metadata":{"id":"9KYGByemRxWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# h5_PATH = '/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA/save_model/epoch30/joint_bert_model.h5'"],"metadata":{"id":"M4Z-vR_JSR8i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bert_hub_PATH = '/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA/albert-module'"],"metadata":{"id":"GfYKGiEUWIN3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["model을 구성하기 위해, sess 라는 파라미터가 필요한데, 이는, train_joint_bert.py 파일 즉, 학습시에 직접 지정해 준 값을 이용하여 sess를 정의하였습니다."],"metadata":{"id":"E3jChzLiRIi9"}},{"cell_type":"code","source":["config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=0, \n","                        inter_op_parallelism_threads=0,\n","                        allow_soft_placement=True,\n","                        device_count = {'GPU': 1})\n","sess = tf.compat.v1.Session(config=config)"],"metadata":{"id":"lIRCrFcehqDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = joint_bert.JointBertModel(slots_num = 2, intents_num = 6, bert_hub_path = bert_hub_PATH, sess = sess, num_bert_fine_tune_layers=10, is_bert= False)"],"metadata":{"id":"xt4f7bEjoYM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trained_PATH = '/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA/save_model/epoch30'"],"metadata":{"id":"z7on41Ye5QPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래 함수를 그대로 이용하면, /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py 내에, decode 관련 오류가 납니다. 이를 직접 위의 로케이션으로 접근하여, 633번줄에 `def load_weights_from_hdf5_group(f, layers):` 아래에, 645, 649 번의 .decode('utf8') 부분을 encode().decode('utf8')로 변경 한 후, 컨트롤+s를 눌러 저장을 하고 아래 코드를 돌려 오류를 수정하였습니다. (이 역시도 버전 충돌 문제라고 생각하지만 임시로 이렇게 수정하였더니 아래 코드에 오류가 나지 않았습니다.)"],"metadata":{"id":"h3MuCpkGRl3R"}},{"cell_type":"markdown","source":["만일, 또 오류시, 런타임 다시시작 후 다시 돌려봅니다."],"metadata":{"id":"qKA7DlTAZCzv"}},{"cell_type":"code","source":["load_model = joint_bert_fixed.JointBertModel.load(trained_PATH, sess)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":327},"id":"kxPb9iB94-ki","executionInfo":{"status":"error","timestamp":1653762836301,"user_tz":-540,"elapsed":2018,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"695d4caa-de77-4a17-9249-c36f0b142593"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-eab2bb8794ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_bert_fixed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJointBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA/models/joint_bert_fixed.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(load_folder_path, sess)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mis_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_bert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJointBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslots_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintents_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_hub_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bert_fine_tune_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_folder_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'joint_bert_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA/models/joint_bert_fixed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, slots_num, intents_num, bert_hub_path, sess, num_bert_fine_tune_layers, is_bert, is_training)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 }\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA/models/joint_bert_fixed.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m            \u001b[0mfine_tune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_bert_fine_tune_layers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m            \u001b[0malbert_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_hub_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m            pooling='mean', name='AlbertLayer')(bert_inputs)\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mintents_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintents_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'intent_classifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_pooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/AIFFELTHON/AIHub감성대화_Data/AI 모델/최종모델링/NIA/layers/albert_layer.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         self.albert = hub.Module(\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malbert_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{self.name}_module\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    177\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m           tags=self._tags)\n\u001b[0m\u001b[1;32m    180\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[0;34m(self, name, trainable, tags)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# TPU training code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[1;32m    456\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         import_scope=relative_scope_name)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;31m# Build a list from the variable name in the module definition to the actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."]}]},{"cell_type":"markdown","source":["다음 과정으로는, 이제 이 모델을 이용하여, 특정 값을 input 하여 감정 분석 결과를 output 하는 함수를 아래의 코드를 분석하여 저희가 직접 짜야 하는것으로 판단하였습니다."],"metadata":{"id":"5mWsKzIu_c41"}},{"cell_type":"markdown","source":["아래는, eval_joint_bert.py파일(테스트 구동 파일로 추정 됨) 내에서 model을 load 한 이후의 코드입니다. 아래 과정에서 어떤 값을 input 하여야하는지, 그리고 이 output을 어떻게 저희 것으로 만들어 내는지가 다음 과정이라 생각하여 아래 코드 구조를 분석 할 예정입니다."],"metadata":{"id":"1dtiAj4XSPWB"}},{"cell_type":"code","source":["# data_text_arr, data_tags_arr, data_intents = Reader.read(data_folder_path)\n","data_text_arr, data_tags_arr, data_intents = Reader.read_allsents(data_folder_path)\n","data_input_ids, data_input_mask, data_segment_ids, data_sequence_lengths = bert_vectorizer.transform(data_text_arr)\n","\n","tags_vectorizer = TagsVectorizer()\n","tags_vectorizer.fit(data_tags_arr)\n","data_tags_arr = tags_vectorizer.transform(data_tags_arr, data_input_ids)\n","\n","#print(data_tags_arr[1])\n","#print(data_input_ids[1])\n","\n","def get_results(input_ids, input_mask, segment_ids,  sequence_lengths, tags_arr,\n","                intents, tags_vectorizer, intents_label_encoder):\n","    inferred_tags, first_inferred_intent, first_inferred_intent_score, _, _, slots_score = model.predict_slots_intent([input_ids, input_mask, segment_ids], tags_vectorizer, intents_label_encoder)\n","    gold_tags = tags_vectorizer.simple_inverse_transform(tags_arr.astype(int), input_ids)\n","    #print(inferred_tags[1])\n","    #print(gold_tags[1])\n","    acc = metrics.accuracy_score(intents, first_inferred_intent)\n","    tag_incorrect = ''\n","    intent_incorrect = ''\n","    intent_correct = ''\n","\n","    for i, sent in enumerate(input_ids):\n","        if intents[i] != first_inferred_intent[i]:\n","            tokens = tokenizer.convert_ids_to_tokens(input_ids[i])\n","            intent_incorrect += ('sent {}\\n'.format(tokens))\n","            intent_incorrect += ('pred: {}\\n'.format(first_inferred_intent[i].strip()))\n","            intent_incorrect += ('score: {}\\n'.format(first_inferred_intent_score[i]))\n","            intent_incorrect += ('ansr: {}\\n'.format(intents[i].strip()))\n","        else:\n","            tokens = tokenizer.convert_ids_to_tokens(input_ids[i])\n","            intent_correct += ('sent {}\\n'.format(tokens))\n","            intent_correct += ('pred: {}\\n'.format(first_inferred_intent[i].strip()))\n","            intent_correct += ('score: {}\\n'.format(first_inferred_intent_score[i]))\n","            intent_correct += ('ansr: {}\\n'.format(intents[i].strip()))\n","\n","\n","    # f1_score\n","    global positive_value\n","    positive_value = 0.5\n","    pv = positive_value\n","    tp = 0\n","    tn = 0\n","    fp = 0\n","    fn = 0\n","\n","    tp_sents = ''\n","    tn_sents = ''\n","    fp_sents = ''\n","    fn_sents = ''\n","\n","    for i in range(len(intents)):\n","        if first_inferred_intent[i] == intents[i] and first_inferred_intent_score[i] >= pv:\n","                tp += 1\n","                tp_sents += ('sent {}\\n'.format(tokens))\n","                tp_sents += ('pred: {}\\n'.format(first_inferred_intent[i].strip()))\n","                tp_sents += ('score: {}\\n'.format(first_inferred_intent_score[i]))\n","                tp_sents += ('ansr: {}\\n'.format(intents[i].strip()))\n","\n","        elif first_inferred_intent[i] != intents[i] and first_inferred_intent_score[i] >= pv:\n","                fp += 1\n","                fp_sents += ('sent {}\\n'.format(tokens))\n","                fp_sents += ('pred: {}\\n'.format(first_inferred_intent[i].strip()))\n","                fp_sents += ('score: {}\\n'.format(first_inferred_intent_score[i]))\n","                fp_sents += ('ansr: {}\\n'.format(intents[i].strip()))\n","\n","        elif first_inferred_intent[i] == intents[i] and first_inferred_intent_score[i] < pv:\n","                fn += 1\n","                fn_sents += ('sent {}\\n'.format(tokens))\n","                fn_sents += ('pred: {}\\n'.format(first_inferred_intent[i].strip()))\n","                fn_sents += ('score: {}\\n'.format(first_inferred_intent_score[i]))\n","                fn_sents += ('ansr: {}\\n'.format(intents[i].strip()))\n","\n","        elif first_inferred_intent[i] != intents[i] and first_inferred_intent_score[i] < pv:\n","                tn += 1\n","                tn_sents += ('sent {}\\n'.format(tokens))\n","                tn_sents += ('pred: {}\\n'.format(first_inferred_intent[i].strip()))\n","                tn_sents += ('score: {}\\n'.format(first_inferred_intent_score[i]))\n","                tn_sents += ('ansr: {}\\n'.format(intents[i].strip()))\n","\n","    precision = tp / (tp + fp)\n","    recall = tp / (tp + fn)\n","    f1_score = 2 * (precision * recall) / (precision + recall)\n","    f1_score = round(f1_score, 3)\n","    precision = round(precision, 3)\n","    recall = round(recall, 3)\n","\n","    return f1_score, precision, recall, acc, intent_incorrect, intent_correct, tp, tn, fp, fn, tp_sents, tn_sents, fp_sents, fn_sents"],"metadata":{"id":"vRfPOWvCSZsN"},"execution_count":null,"outputs":[]}]}